# Machine Learning & Data Science Consultant

I am a Data Science Consultant at QuantumBlack, AI by McKinsey, where I help organizations transform their operations through advanced analytics. I work client-facing on end-to-end advanced machine learning projects across several industries (e.g., k-means customer segmentation pipelines, time-series forecasting models, etc.). Much of my work combines technical implementation, stakeholder alignment, and translating complex analytics into actionable recommendations.

## Background

I hold a Master's degree in Engineering and Management from the University of Cambridge, building on a Bachelor's in Mechanical Engineering from Mannheim.

As a Data Science Consultant at QuantumBlack, McKinsey's AI and Machine Learning arm, I work across Consumer, Pharma, and TMT clients, contributing to projects that combine analytics, strategy, and product thinking. In this client-facing role, I collaborate with engineers, product teams, and senior stakeholders to translate analytical insights into clear recommendations and strategic materials that support decision-making on high-impact initiatives.

Previously, as Consultant in McKinsey's Operations Practice, I supported an analytics workstream for a German advanced-industries client, contributing to strategic value analysis, market benchmarking, and operational improvement efforts. At the University of Cambridge, I worked on forecasting and strategic modeling projects for manufacturing SMEs and startups, helping clients translate data into commercial and operational strategies. Earlier, during my trainee program at ABB, I gained hands-on experience in process improvement and operational analysis across engineering and supply-chain functions.

These experiences have reinforced my ambition to work at the intersection of strategy, technology, and product innovation.

## What I Do

- **End-to-end ML pipelines**: Design and implement production-ready data science workflows from raw data ingestion to model deployment
- **Experimentation**: Develop robust experimentation frameworks for model selection, hyperparameter tuning, and A/B testing
- **Explainability**: Apply SHAP, feature importance, and other interpretability techniques to make model decisions transparent to stakeholders
- **Deployment patterns**: Containerize models, set up cloud infrastructure, and build monitoring systems for model performance tracking
- **Client collaboration**: Coach technical teams, translate technical concepts for executive audiences, and drive analytics migrations

## Selected Work

- [Customer Segmentation](./projects/segmentation.md) — Retail healthcare client: modular preprocessing pipeline, K-means clustering, and Gradient Boosting to extract purchase-behavior drivers
- [Demand Forecasting](./projects/forecasting.md) — Pharma client: univariate and multivariate forecasting with external time-series integration, explainability via SHAP
- [Defect Forecasting](./projects/defect_forecasting.md) — Manufacturing analytics client: classification model for time-series defect prediction, evaluation and explainability, AWS experimentation, Dockerized pipeline

## Toolbox

**Languages & Frameworks**: Python, R, SQL  
**ML & Data**: Jupyter, Kedro, scikit-learn, XGBoost, SHAP  
**Cloud & Infrastructure**: AWS, GCP, Docker  
**Visualization & BI**: Power BI  
**Engineering**: Git, ANSYS  
**Other**: MLOps practices, time-series analysis, classification, clustering

## Contact

- Email: <max_haehn@gmx.de>
- LinkedIn: <linkedin.com/in/max-haehn>
